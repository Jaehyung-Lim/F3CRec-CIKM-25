{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def save_pickle(file_path, obj):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = './ml-100k.rating.data'\n",
    "columns = ['user', 'item', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv(file_path, sep='\\t', names=columns, engine='python')\n",
    "\n",
    "# Step 1\n",
    "positive_ratings = ratings[ratings['rating'] > 0]\n",
    "\n",
    "# Step 2: Apply k-core filtering\n",
    "def filter_k_core(df, k=10):\n",
    "    while True:\n",
    "        before_shape = df.shape[0]\n",
    "        user_counts = df['user'].value_counts()\n",
    "        item_counts = df['item'].value_counts()\n",
    "        \n",
    "        df = df[df['user'].isin(user_counts[user_counts >= k].index)]\n",
    "        df = df[df['item'].isin(item_counts[item_counts >= k].index)]\n",
    "        \n",
    "        after_shape = df.shape[0]\n",
    "        if before_shape == after_shape:\n",
    "            break\n",
    "    return df\n",
    "\n",
    "# K: 10\n",
    "filtered_ratings = filter_k_core(positive_ratings, k=10)\n",
    "\n",
    "# Step 3: Sort the data by timestamp\n",
    "filtered_ratings = filtered_ratings.sort_values(by='timestamp')\n",
    "\n",
    "# Step 4: Remap user and item IDs starting from 0\n",
    "unique_user_ids = filtered_ratings['user'].unique()\n",
    "unique_item_ids = filtered_ratings['item'].unique()\n",
    "\n",
    "user_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_user_ids)}\n",
    "item_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_item_ids)}\n",
    "\n",
    "filtered_ratings['user'] = filtered_ratings['user'].map(user_id_map)\n",
    "filtered_ratings['item'] = filtered_ratings['item'].map(item_id_map)\n",
    "\n",
    "# Assign unique interaction IDs starting from 0\n",
    "filtered_ratings['interactionId'] = range(len(filtered_ratings))\n",
    "\n",
    "# Step 5: Determine splits based on interaction counts\n",
    "total_interactions = len(filtered_ratings)\n",
    "base_count = int(total_interactions * 0.6)\n",
    "inc1_count = int(total_interactions * 0.4/3)\n",
    "inc2_count = int(total_interactions * 0.4/3)\n",
    "inc3_count = total_interactions - (base_count + inc1_count + inc2_count)\n",
    "\n",
    "# Step 6: Split the data based on interaction counts\n",
    "base_data = filtered_ratings.iloc[:base_count]\n",
    "inc1_data = filtered_ratings.iloc[base_count:base_count + inc1_count]\n",
    "inc2_data = filtered_ratings.iloc[base_count + inc1_count:base_count + inc1_count + inc2_count]\n",
    "inc3_data = filtered_ratings.iloc[base_count + inc1_count + inc2_count:]\n",
    "\n",
    "# Step 7: Split train-valid-test in each user's interaction for base data\n",
    "def split_base_data(df, train_frac=8, val_frac=1, test_frac=1):\n",
    "    user_groups = df.groupby('user')\n",
    "    train_set = pd.DataFrame()\n",
    "    val_set = pd.DataFrame()\n",
    "    test_set = pd.DataFrame()\n",
    "    \n",
    "    for user, group in user_groups:\n",
    "        if len(group) < 3:\n",
    "            train_set = pd.concat([train_set, group])\n",
    "            continue\n",
    "        \n",
    "        test_size = test_frac / (train_frac + val_frac + test_frac)\n",
    "        val_size = val_frac / (train_frac + val_frac)\n",
    "        \n",
    "        train_val_data, test_data = train_test_split(group, test_size=test_size, shuffle=True)\n",
    "        train_data, val_data = train_test_split(train_val_data, test_size=val_size, shuffle=True)\n",
    "        \n",
    "        train_set = pd.concat([train_set, train_data])\n",
    "        val_set = pd.concat([val_set, val_data])\n",
    "        test_set = pd.concat([test_set, test_data])\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Step 8: Split each incremental block into validation and test sets\n",
    "\n",
    "def split_inc_block_userwise(df):\n",
    "    user_groups = df.groupby('user')\n",
    "    train_set = pd.DataFrame()\n",
    "    val_set = pd.DataFrame()\n",
    "    test_set = pd.DataFrame()\n",
    "\n",
    "    for user, group in user_groups:\n",
    "        if len(group) < 3:\n",
    "            train_set = pd.concat([train_set, group])\n",
    "            continue\n",
    "        \n",
    "        test_size = 1/10\n",
    "        valid_size = 1/9\n",
    "        \n",
    "        train_val_data, test_data = train_test_split(group, test_size = test_size, shuffle=False)\n",
    "        train_data, val_data = train_test_split(train_val_data, test_size = valid_size, shuffle=False)\n",
    "\n",
    "        train_set = pd.concat([train_set, train_data])\n",
    "        val_set = pd.concat([val_set, val_data])\n",
    "        test_set = pd.concat([test_set, test_data])\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "total_block = [base_data, inc1_data, inc2_data, inc3_data]\n",
    "train_base, val_base, test_base = split_base_data(base_data)\n",
    "train_inc1, val_inc1, test_inc1 = split_inc_block_userwise(inc1_data) \n",
    "train_inc2, val_inc2, test_inc2 = split_inc_block_userwise(inc2_data)\n",
    "train_inc3, val_inc3, test_inc3 = split_inc_block_userwise(inc3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(base_data))\n",
    "print(len(inc1_data))\n",
    "print(len(inc2_data))\n",
    "print(len(inc3_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_ratings['user'].max())\n",
    "print(filtered_ratings['item'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_0 = {'train': train_base, 'valid': val_base, 'test': test_base}\n",
    "TASK_1 = {'train': train_inc1, 'valid': val_inc1, 'test': test_inc1}\n",
    "TASK_2 = {'train': train_inc2, 'valid': val_inc2, 'test': test_inc2}\n",
    "TASK_3 = {'train': train_inc3, 'valid': val_inc3, 'test': test_inc3}\n",
    "\n",
    "TASK = [TASK_0, TASK_1, TASK_2, TASK_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle('./total_blocks_timestamp.pickle', total_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_list = []\n",
    "\n",
    "# for idx, task in enumerate(TASK):\n",
    "#     train_dict = {}\n",
    "#     valid_dict = {}\n",
    "#     test_dict = {}\n",
    "    \n",
    "#     res_item_list = []\n",
    "    \n",
    "#     # train\n",
    "#     train = task['train']\n",
    "#     user_groups = train.groupby('user')\n",
    "    \n",
    "#     for user, group in user_groups:\n",
    "#         item_list = group['item'].values.tolist()\n",
    "#         train_dict[user] = item_list\n",
    "#         res_item_list.extend(item_list)\n",
    "    \n",
    "#     # valid\n",
    "#     valid = task['valid']\n",
    "#     user_groups = valid.groupby('user')\n",
    "    \n",
    "#     for user, group in user_groups:\n",
    "#         item_list = group['item'].values.tolist()\n",
    "#         valid_dict[user] = item_list\n",
    "#         res_item_list.extend(item_list)\n",
    "    \n",
    "#     # test\n",
    "#     test = task['test']\n",
    "#     user_groups = test.groupby('user')\n",
    "    \n",
    "#     for user, group in user_groups:\n",
    "#         item_list = group['item'].values.tolist()\n",
    "#         test_dict[user] = item_list\n",
    "#         res_item_list.extend(item_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     TASK_pickle = {'train_dict': train_dict, 'valid_dict': valid_dict, 'test_dict': test_dict, 'item_list': res_item_list}\n",
    "    \n",
    "#     pickle_list.append(TASK_pickle)\n",
    "    \n",
    "#     save_pickle(f'./TASK_{idx}.pickle', TASK_pickle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
